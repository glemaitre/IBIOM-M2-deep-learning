{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The three ingredients of a classifier: the model, the loss, and the optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use extensively numpy, pandas, and matplotlib libraries over the lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Let's build a simple dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first take a subset of data of the iris dataset. It will simplify and allow to come with some intuitions. We will select the sepal length and width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:, :2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we will use only the samples corresponding to the class `0` and `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_class_0_1 = np.bitwise_or(y == 0, y == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[mask_class_0_1]\n",
    "y = y[mask_class_0_1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make a scatter of those data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.xlabel(iris.feature_names[0])\n",
    "plt.ylabel(iris.feature_names[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The model: case of a linear classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classifier can be seen as a function $f(\\cdot)$ such that given $\\mathbf{x}$ (i.e., a row of the matrix `X`) will return $y$ (i.e., the class of the flower). So basically, we have $f(\\mathbf{x}) = y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we deal with a linear classifier, it means that the function $f(\\mathbf{x})$ is a linear combination of the input features (i.e., sepal width and sepal length). Therefore, we can formulate this as $f(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x}$. Sometimes, an extra parameter is included avoiding the function to pass by the origin: the bias. The function is then represented by $f(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, in our previous example it means that $f(\\mathbf{X}) = 0$ is the line which separate the data into two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "* Define a pair of points which separate the two classes. Plot a line joining these points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/01_01.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/01_02.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recall that a linear classifier is defined the function $f(\\mathbf{X}) = \\mathbf{w} \\cdot \\mathbf{X} + b$\n",
    "\n",
    "* From the pair of points that you defined previously, how do you compute the associated weight with the decsion function defined by the pair of points? Hint: You can look at `numpy.linalg.solve`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/01_03.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also inverse the function to find $\\mathbf{w}$ such that $\\mathbf{w} = \\mathbf{X}^{-1} \\cdot \\mathbf{b}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = np.dot(np.linalg.inv(decision_points),\n",
    "               np.ones(shape=(decision_points.shape[0], 1)))\n",
    "coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using the coefficients that you computed, define a function which would compute the distance to the line. Hint: this is basically the function of a linear classifier (https://en.wikipedia.org/wiki/Linear_predictor_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/01_04.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Take the first sample of the dataset and compute the distance to the line. Check also the sign which will let you know on which side of the line the point will lie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/01_05.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, knowing these coefficient we can actually create a linear classifier which can automatically tell us on which side of the line we are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManualLinearClassifier:\n",
    "    def __init__(self, decision_points):\n",
    "        self.decision_points = decision_points\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        dummy_feature = np.ones(shape=(self.decision_points.shape[0], 1))\n",
    "        self.intercept_ = 1\n",
    "        self.coefs_ = np.linalg.solve(self.decision_points, dummy_feature)\n",
    "        \n",
    "    def decision_function(self, X):\n",
    "        return (np.dot(X, self.coefs_) - self.intercept_).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we can create a classifier and call the appropriate function to compute the distance to the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ManualLinearClassifier(decision_points)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.decision_function(np.array([[5.0, 2.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the line corresponding to $\\mathbf{w} \\cdot \\mathbf{X} + b = 0$ which should be our line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d_separator(classifier, X, ax=None, levels=None, eps=None):\n",
    "    eps = X.std() / 2. if eps is None else eps\n",
    "    levels = [0] if levels is None else levels\n",
    "    ax = plt.gca() if ax is None else ax\n",
    "    \n",
    "    x_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\n",
    "    y_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps\n",
    "    xx = np.linspace(x_min, x_max, 100)\n",
    "    yy = np.linspace(y_min, y_max, 100)\n",
    "\n",
    "    X1, X2 = np.meshgrid(xx, yy)\n",
    "    X_grid = np.c_[X1.ravel(), X2.ravel()]\n",
    "    decision_values = classifier.decision_function(X_grid)\n",
    "    CS = ax.contour(X1, X2, decision_values.reshape(X1.shape), levels=levels)\n",
    "    ax.clabel(CS, inline=1, fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.xlabel(iris.feature_names[0])\n",
    "plt.ylabel(iris.feature_names[1]);\n",
    "plot_2d_separator(clf, X, levels=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot different distance ranging from -5 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.xlabel(iris.feature_names[0])\n",
    "plt.ylabel(iris.feature_names[1]);\n",
    "plot_2d_separator(clf, X, levels=np.arange(-5, 5, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Modify the previous classifier class to add a `predict_proba` function. This function should apply a sigmoid function on the output of the `decision_function` to output probabilities. Beaware that we expect the probablity to belong to the class 0 and to the class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/01_06.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ManualLinearClassifier(decision_points)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.xlabel(iris.feature_names[0])\n",
    "plt.ylabel(iris.feature_names[1]);\n",
    "plot_2d_separator(clf, X, levels=np.arange(-5, 5, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Decision function: ',\n",
    "      clf.decision_function(np.array([[5.0, 2.0]])))\n",
    "print('Probability: ',\n",
    "      clf.predict_proba(np.array([[5.0, 2.0]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Implement a `predict` method which should threshold the probability and output only the correct label. You can use the sign of the `decision_function` for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/01_07.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ManualLinearClassifier(decision_points)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.xlabel(iris.feature_names[0])\n",
    "plt.ylabel(iris.feature_names[1]);\n",
    "plot_2d_separator(clf, X, levels=np.arange(-5, 5, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Decision function: ',\n",
    "      clf.decision_function(np.array([[5.0, 2.0]])))\n",
    "print('Probability: ',\n",
    "      clf.predict_proba(np.array([[5.0, 2.0]])))\n",
    "print('Label: ',\n",
    "      clf.predict(np.array([[5.0, 2.0]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compute the accuracy of our classifier on the whole dataset. You can import the function `sklearn.metrics.accuracy_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/01_08.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The duo loss-optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now, we create a classifier and define the \"decision rule\" by hand. However, machine learning provides algorithms and tools to learn these decision rules directly from the data. Indeed, this learning problem can be formulated as an optimization problem: we use an optimizer to find the best set of parameters of our model which reduce a cost function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Our loss function: negative log likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a classificatiion problem, we can use the maximum likelihood principle. Indeed, we are interested in maximizing the following:\n",
    "\n",
    "$\\mathbf{\\hat{w}} = \\underset{\\mathbf{w}}{\\text{argmax}} P(\\mathbf{Y} | \\mathbf{X}; \\mathbf{w})\\ .$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In plain English, we are interested in finding the parameters which give rise to our data. Under some assumptions, the maximum likelihood estimate is equal to:\n",
    "\n",
    "$\\mathbf{\\hat{w}} = \\underset{\\mathbf{w}}{\\text{argmax}} \\prod_{i=1}^{m} P(\\mathbf{y}_{i} | \\mathbf{x}_{i}; \\mathbf{w})\\ .$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the product is not convenient and we can replace by a sum by taking the log of the maximum likelihood:\n",
    "\n",
    "$\\mathbf{\\hat{w}} = \\underset{\\mathbf{w}}{\\text{argmax}} \\sum_{i=1}^{m} \\log P(\\mathbf{y}_{i} | \\mathbf{x}_{i}; \\mathbf{w})\\ .$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, maximizing the log likelihood is equivalent to minimizing the negative log likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing that for the logistic regression, we have $P(\\mathbf{y}_{i} | \\mathbf{x}_{i}; \\mathbf{w}) = (\\sigma(\\mathbf{x}_{i}))^{\\mathbf{y}_{i}} (1 - \\sigma(\\mathbf{x}_{i}))^{1 - \\mathbf{y}_{i}}$. $\\sigma(\\cdot)$ is the sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, our the negative log likelihood is defined as:\n",
    "\n",
    "$l(\\mathbf{w}) = - \\sum_{i=1}^{m} \\mathbf{y}_{i} \\log \\sigma(\\mathbf{x}_{i}) + (1 - \\mathbf{y}_{i}) \\log (1 - \\sigma(\\mathbf{x}_{i}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This metric is implemented in `sklearn.metrics.log_loss`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Our optimizer: stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To minimize a given function, one can use the gradient descent algorithm. For our problem, we want to iteratively update our set of our model parameters such that the function $l(\\mathbf{w})$ is decreasing. An update in the gradient descent is formulated as:\n",
    "\n",
    "$\\mathbf{w} := \\mathbf{w} - \\frac{\\alpha}{m} \\Delta_{\\mathbf{w}}l \\ ,$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\alpha$ is the learning rate and $m$ is the number of samples used for computed the derivative of the loss. The derivative of the negative log likelihood is defined as:\n",
    "\n",
    "$\\Delta_{\\mathbf{w}}l = \\mathbf{x}^{T} \\cdot (\\sigma(\\mathbf{x}) - \\mathbf{y}) \\ .$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic gradient descent differs from a gradient descent by evaluating the gradient and updating the parameter with a single sample. Note that it can be used in conjunction with mini-batches of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "* Create a function called `grad_nll` which given `X`, `y`, an `coefs` compute the derivative of the log likelihood as defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/01_09.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a function called `sgd_train`. Pass `X`, `y`, `batch_size`, `max_iter`, and `learning_rate` as parameter.\n",
    "* Initialize the weight with random values. The size is defined by the number of features in `X`.\n",
    "* In an infinite loop, do:\n",
    "    * Select a subset of data defined by a variable `batch_size`,\n",
    "    * Iterate over each sample of the mini-batch and,\n",
    "    * Compute the gradient of the negative log likelihood using the above function,\n",
    "    * increase the number of iteration.\n",
    "* Stop iterating after `max_iter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/01_10.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will check if your optimization is working. Let's compute the coefficients using the `sgd_train` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = sgd_train(X, y, batch_size=20, max_iter=1000, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will reuse our old classifier and set the coefficients an instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ManualLinearClassifier(decision_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.coefs_ = coefs[:2]\n",
    "clf.intercept_ = coefs[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally check what decision boundaries did we learn with our logistic regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.xlabel(iris.feature_names[0])\n",
    "plt.ylabel(iris.feature_names[1]);\n",
    "plot_2d_separator(clf, X, levels=np.arange(-5, 5, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Our final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self, learning_rate=0.1, max_iter=100, tol=1e-3,\n",
    "                 batch_size=20):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def _sigmoid(self, X):\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "    \n",
    "    def _decision_function(self, X):\n",
    "        return np.dot(X, self.coefs_)\n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        X = self._add_intercept(X)\n",
    "        return self._decision_function(X).ravel()\n",
    "    \n",
    "    def _grad_nll(self, X, y):\n",
    "        grad = (self._predict_proba(X) - y)\n",
    "        return np.dot(X.T, grad)\n",
    "    \n",
    "    def _add_intercept(self, X):\n",
    "        return np.hstack((X, np.ones(shape=(X.shape[0], 1))))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = self._add_intercept(X)\n",
    "        # Make y to be a column vector for later operation\n",
    "        y = np.atleast_2d(y).T\n",
    "        # Initialize randomly the weights\n",
    "        self.coefs_ = np.random.rand(X.shape[1], 1)\n",
    "        \n",
    "        it = 0\n",
    "        loss = np.inf\n",
    "        while it < self.max_iter and loss > self.tol:\n",
    "            # select a minibatch\n",
    "            idx = np.random.choice(np.arange(X.shape[0]),\n",
    "                                   size=self.batch_size)\n",
    "            X_subset, y_subset = X[idx], y[idx]\n",
    "            for sample_idx in range(X_subset.shape[0]):\n",
    "                # compute the gradient\n",
    "                dnll = self._grad_nll(X_subset[[sample_idx]], y_subset[[sample_idx]])\n",
    "                # update the parameter\n",
    "                self.coefs_ -= self.learning_rate * dnll\n",
    "            # update the loss and the number of iteration\n",
    "            loss = log_loss(y, self._predict_proba(X))\n",
    "            it += 1\n",
    "        return self\n",
    "    \n",
    "    def _predict_proba(self, X):\n",
    "        return self._sigmoid(self._decision_function(X))\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X = self._add_intercept(X)\n",
    "        return self._predict_proba(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        prob = self.predict_proba(X)\n",
    "        prob[prob < 0.5] = 0\n",
    "        prob[prob >= 0.5] = 1\n",
    "        return prob.astype(int).ravel()\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        return np.mean(y == self.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(learning_rate=0.1)\n",
    "clf.fit(X, y).score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.xlabel(iris.feature_names[0])\n",
    "plt.ylabel(iris.feature_names[1]);\n",
    "plot_2d_separator(clf, X, levels=np.arange(-5, 5, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
